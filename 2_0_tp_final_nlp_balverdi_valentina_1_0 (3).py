# -*- coding: utf-8 -*-
"""2_0_TP_FINAL_NLP_Balverdi_Valentina_1_0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qup9-bowHhV7wlMq4QWndAC-Q92Xb701

# **Trabajo Pr√°ctico Final 2025 - Procesamiento del Lenguaje Natural - TUIA**

**Alumna**: Valentina Balverdi

**Profesores:**
* Juan Pablo Manson
* Alan Geray
* Constantino Ferrucci
"""

# models/embedding-gecko-001
# models/gemini-2.5-flash
# models/gemini-2.5-pro
# models/gemini-2.0-flash-exp
# models/gemini-2.0-flash
# models/gemini-2.0-flash-001
# models/gemini-2.0-flash-exp-image-generation
# models/gemini-2.0-flash-lite-001
# models/gemini-2.0-flash-lite
# models/gemini-2.0-flash-lite-preview-02-05
# models/gemini-2.0-flash-lite-preview
# models/gemini-2.0-pro-exp
# models/gemini-2.0-pro-exp-02-05
# models/gemini-exp-1206
# models/gemini-2.5-flash-preview-tts
# models/gemini-2.5-pro-preview-tts
# models/gemma-3-1b-it
# models/gemma-3-4b-it
# models/gemma-3-12b-it
# models/gemma-3-27b-it
# models/gemma-3n-e4b-it
# models/gemma-3n-e2b-it
# models/gemini-flash-latest
# models/gemini-flash-lite-latest
# models/gemini-pro-latest
# models/gemini-2.5-flash-lite
# models/gemini-2.5-flash-image-preview
# models/gemini-2.5-flash-image
# models/gemini-2.5-flash-preview-09-2025
# models/gemini-2.5-flash-lite-preview-09-2025
# models/gemini-3-pro-preview
# models/gemini-3-pro-image-preview
# models/nano-banana-pro-preview
# models/gemini-robotics-er-1.5-preview
# models/gemini-2.5-computer-use-preview-10-2025
# models/embedding-001
# models/text-embedding-004
# models/gemini-embedding-exp-03-07
# models/gemini-embedding-exp
# models/gemini-embedding-001
# models/aqa
# models/imagen-4.0-generate-preview-06-06
# models/imagen-4.0-ultra-generate-preview-06-06
# models/imagen-4.0-generate-001
# models/imagen-4.0-ultra-generate-001
# models/imagen-4.0-fast-generate-001
# models/veo-2.0-generate-001
# models/veo-3.0-generate-001
# models/veo-3.0-fast-generate-001
# models/veo-3.1-generate-preview
# models/veo-3.1-fast-generate-preview
# models/gemini-2.0-flash-live-001
# models/gemini-live-2.5-flash-preview
# models/gemini-2.5-flash-live-preview
# models/gemini-2.5-flash-native-audio-latest
# models/gemini-2.5-flash-native-audio-preview-09-2025

modelo_gemini = "gemini-2.0-flash"

"""### Descarga de archivos desde Drive"""

import os
import glob
import json
import numpy
import pandas as pd
import re

"""#### CSV a documento"""

from google.colab import drive
drive.mount('/content/drive')

BASE_PATH = "/content/drive/MyDrive/TUIA/NLP/TP_FINAL/fuentes_de_informacion"

# Carga de archivos tabulares
path = BASE_PATH

devoluciones = pd.read_csv(os.path.join(path, "devoluciones.csv"))
inventario = pd.read_csv(os.path.join(path, "inventario_sucursales.csv"))
productos = pd.read_csv(os.path.join(path, "productos.csv"))
ventas = pd.read_csv(os.path.join(path, "ventas_historicas.csv"))
vendedores = pd.read_csv(os.path.join(path, "vendedores.csv"))
tickets = pd.read_csv(os.path.join(path, "tickets_soporte.csv"))

with open(os.path.join(path, "faqs.json"), "r", encoding="utf-8") as f:
    faqs = json.load(f)

"""#### TXT a documento"""

# Carga de textos
resenas_dir = os.path.join(path, "resenas_usuarios")
reviews_files = glob.glob(os.path.join(resenas_dir, "*.txt"))

reviews_docs = []

for fp in reviews_files:
    with open(fp, "r", encoding="utf-8") as f:
        text = f.read()

    # Separar encabezado de cuerpo (una l√≠nea en blanco entre ambos)
    partes = text.split("\n\n", 1)
    header = partes[0]
    cuerpo = partes[1] if len(partes) > 1 else ""

    # Inicializar campos
    fecha = usuario = telefono = producto_nombre = producto_id = puntaje = provincia = None

    for line in header.splitlines():
        line = line.strip()
        if line.startswith("Fecha:"):
            fecha = line.replace("Fecha:", "").strip()
        elif line.startswith("Usuario:"):
            usuario = line.replace("Usuario:", "").strip()
        elif line.startswith("Tel√©fono:"):
            telefono = line.replace("Tel√©fono:", "").strip()
        elif line.startswith("Producto:"):
            # Ej: "Producto: Profesional Batidora de Mano (P0017)"
            m = re.match(r"Producto:\s*(.+)\s+\((P\d+)\)", line)
            if m:
                producto_nombre = m.group(1).strip()
                producto_id = m.group(2).strip()
        elif line.startswith("Puntaje:"):
            # Ej: "Puntaje: 5/5"
            puntaje = line.replace("Puntaje:", "").strip()
        elif line.startswith("Provincia:"):
            provincia = line.replace("Provincia:", "").strip()

    reviews_docs.append({
        "content": cuerpo.strip(),              # texto que se embebe
        "source": os.path.basename(fp),
        "type": "review",
        "fecha": fecha,
        "usuario": usuario,
        "telefono": telefono,
        "producto_id": producto_id,
        "producto_nombre": producto_nombre,
        "puntaje": puntaje,
        "provincia": provincia,
    })

"""#### .md y JSON a documento"""

# Manuales .md
manuales_dir = os.path.join(path, "manuales_productos")
manuales_files = glob.glob(os.path.join(manuales_dir, "*.md"))

manuales_docs = []

for fp in manuales_files:
    with open(fp, "r", encoding="utf-8") as f:
        text = f.read()

    # --- Extraer informaci√≥n del encabezado ---
    # T√≠tulo: "# Manual T√©cnico - Compacto Licuadora"
    titulo_match = re.search(r"# Manual T√©cnico - (.+)", text)
    producto_nombre = titulo_match.group(1).strip() if titulo_match else None

    # Modelo: "**Modelo:** P0004 | **Marca:** ChefMaster"
    modelo_match = re.search(r"\*\*Modelo:\*\*\s*([A-Z0-9]+)", text)
    producto_id = modelo_match.group(1).strip() if modelo_match else None

    marca_match = re.search(r"\*\*Marca:\*\*\s*([A-Za-z0-9]+)", text)
    marca = marca_match.group(1).strip() if marca_match else None

    # Nombre Comercial (en Especificaciones T√©cnicas)
    comercial_match = re.search(r"\*\*Nombre Comercial:\*\*\s*(.+)", text)
    nombre_comercial = comercial_match.group(1).strip() if comercial_match else producto_nombre

    # Categor√≠a
    categoria_match = re.search(r"\*\*Categor√≠a:\*\*\s*(.+)", text)
    categoria = categoria_match.group(1).strip() if categoria_match else None

    # Guardar documento con metadata
    manuales_docs.append({
        "content": text,
        "source": os.path.basename(fp),
        "type": "manual",
        "producto_id": producto_id,
        "producto_nombre": nombre_comercial,
        "marca": marca,
        "categoria_producto": categoria
    })

# Convertir FAQs a documentos
faq_docs = []

for item in faqs:
    pregunta = item["pregunta"]
    respuesta = item["respuesta"]
    producto = item["nombre_producto"]
    categoria = item["categoria"]

    faq_docs.append({
        "content": (
            f"PRODUCTO: {producto}\n"
            f"CATEGOR√çA FAQ: {categoria}\n"
            f"PREGUNTA: {pregunta}\n"
            f"RESPUESTA: {respuesta}"
        ),
        "source": f"faq_{item['id_faq']}.json",
        "type": "faq",
        "id_faq": item["id_faq"],
        "id_producto": item["id_producto"],
        "producto_nombre": item["nombre_producto"],
        "categoria_faq": item["categoria"]
    })


# Tickets de soporte (texto largo)
ticket_docs = []

for _, row in tickets.iterrows():
    texto = ""

    # incluir tipo de problema si existe
    if isinstance(row["tipo_problema"], str):
        texto += f"Tipo de problema: {row['tipo_problema']}\n"

    # incluir descripci√≥n si existe
    if isinstance(row["descripcion"], str):
        texto += f"Descripci√≥n: {row['descripcion']}\n"

    # Si tenemos texto v√°lido, lo guardamos
    if texto.strip():
        ticket_docs.append({
            "content": texto.strip(),
            "source": f"ticket_{row['id_ticket']}",
            "type": "ticket_soporte",
            "producto_id": row["id_producto"],
            "producto_nombre": row["nombre_producto"],
            "categoria": row["categoria"],
            "severidad": row["severidad"],
        })

# Union para la base vectorial
text_documents = reviews_docs + manuales_docs + faq_docs + ticket_docs

print("=== FAQs ===")
print("Cantidad de FAQs:", len(faqs))
print("Ejemplo FAQ[0]:")
print(faqs[0])

print("\n=== ticket_docs ===")
print("Cantidad de ticket_docs:", len(ticket_docs))
print("Ejemplo ticket_docs[0]:")
print(ticket_docs[0])

print("\n=== DataFrames tabulares ===")
for nombre, df in {
    "devoluciones": devoluciones,
    "inventario": inventario,
    "productos": productos,
    "ventas": ventas,
    "vendedores": vendedores,
    "tickets_df": tickets,
}.items():
    print(f"\n[{nombre}] shape={df.shape}")
    print("Columnas:", list(df.columns))

def stats_longitudes(docs, nombre):
    longitudes = [len(d["content"]) for d in docs]
    if not longitudes:
        print(f"No hay documentos en {nombre}")
        return None

    serie = pd.Series(longitudes)
    print(f"=== {nombre} ===")
    print("Cantidad de documentos:", len(longitudes))
    print("M√≠nimo:", int(serie.min()))
    print("M√°ximo:", int(serie.max()))
    print("Promedio:", int(serie.mean()))
    print("Mediana:", int(serie.median()))
    print("Percentil 75:", int(serie.quantile(0.75)))
    print("Percentil 90:", int(serie.quantile(0.90)))
    print()
    return serie

len_reviews  = stats_longitudes(reviews_docs,  "Rese√±as")
len_manuales = stats_longitudes(manuales_docs, "Manuales")
len_faqs     = stats_longitudes(faq_docs,      "FAQs")
len_tickets  = stats_longitudes(ticket_docs,   "Tickets soporte")

"""# **Ejercicio 1:** RAG

## **Base de datos Vectorial**
La base de datos vectorial se utiliza para indexar todas las fuentes de informaci√≥n no estructurada o semiestructurada del proyecto, incluyendo:

- Manuales de producto (`.md`)
- Rese√±as de usuarios (`.txt`)
- Preguntas frecuentes (`faqs.json`)
- Descripciones de tickets de soporte (`tickets_soporte.csv`)

Este tipo de contenido es ideal para un esquema (RAG), ya que los usuarios realizan consultas en lenguaje natural y se requiere una b√∫squeda sem√°ntica, no solo coincidencia literal de palabras clave.

### **Fragmentaci√≥n del texto (Text Splitter)**

Para preparar los documentos antes de generar embeddings, se utiliz√≥ el **RecursiveCharacterTextSplitter** de LangChain. Este splitter fue elegido porque:

- Es robusto para textos largos como manuales.
- Intenta mantener unidades sem√°nticas coherentes (p√°rrafos, oraciones) antes de cortar por caracteres.
- Permite controlar el tama√±o del fragmento (*chunk_size*) y el solapamiento (*chunk_overlap*).
- Evita cortes innecesarios en textos cortos como rese√±as, FAQs y tickets.

A partir del an√°lisis de longitudes reales de los documentos:

| Fuente      | P75 longitud |
|-------------|--------------|
| Rese√±as     | 368 chars    |
| FAQs        | 317 chars    |
| Tickets     | 102 chars    |
| Manuales    | 6539 chars   |

Se seleccion√≥:

- **chunk_size = 512** ‚Üí suficientemente grande para mantener intactas rese√±as, FAQs y tickets,  
  pero suficientemente peque√±o para dividir manuales extensos de forma manejable.
- **chunk_overlap = 64** (‚âà 12%) ‚Üí asegura continuidad sem√°ntica entre fragmentos contiguos.
"""

!pip install langchain-text-splitters

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""],
    chunk_size=512,
    chunk_overlap=64,
)

chunks_final = []

# ---- Reviews ----
for i, review_doc in enumerate(reviews_docs):
    chunks = splitter.split_text(review_doc["content"])
    for j, chunk in enumerate(chunks):
        chunks_final.append({
            "id": f"review_{i}_chunk_{j}",
            "content": chunk,
            "metadata": {
                "type": "review",
                "fuente": review_doc["source"],
                "chunk_index": j,
                "producto_id": review_doc.get("producto_id"),
                "producto_nombre": review_doc.get("producto_nombre"),
                "puntaje": review_doc.get("puntaje"),
                "provincia": review_doc.get("provincia"),
            }
        })

# ---- Manuales ----
for i, manual_doc in enumerate(manuales_docs):
    chunks = splitter.split_text(manual_doc["content"])
    for j, chunk in enumerate(chunks):
        chunks_final.append({
            "id": f"manual_{i}_chunk_{j}",
            "content": chunk,
            "metadata": {
                "type": "manual",
                "fuente": manual_doc["source"],
                "chunk_index": j,
                "producto_id": manual_doc.get("producto_id"),
                "producto_nombre": manual_doc.get("producto_nombre"),
                "marca": manual_doc.get("marca"),
                "categoria_producto": manual_doc.get("categoria_producto"),
            }
        })

# ---- FAQs ----
for i, faq_doc in enumerate(faq_docs):
    chunks = splitter.split_text(faq_doc["content"])
    for j, chunk in enumerate(chunks):
        chunks_final.append({
            "id": f"faq_{i}_chunk_{j}",
            "content": chunk,
            "metadata": {
                "type": "faq",
                "fuente": faq_doc["source"],
                "chunk_index": j,
                "id_faq": faq_doc.get("id_faq"),
                "id_producto": faq_doc.get("id_producto"),
                "producto_nombre": faq_doc.get("producto_nombre"),
                "categoria_faq": faq_doc.get("categoria_faq"),
            }
        })

# ---- Tickets ----
for i, ticket_doc in enumerate(ticket_docs):
    chunks = splitter.split_text(ticket_doc["content"])
    for j, chunk in enumerate(chunks):
        chunks_final.append({
            "id": f"ticket_{i}_chunk_{j}",
            "content": chunk,
            "metadata": {
                "type": "ticket",
                "fuente": ticket_doc["source"],
                "chunk_index": j,
                "producto_id": ticket_doc.get("producto_id"),
                "producto_nombre": ticket_doc.get("producto_nombre"),
                "categoria_ticket": ticket_doc.get("categoria"),
                "severidad": ticket_doc.get("severidad"),
            }
        })

print("Cantidad total de chunks:", len(chunks_final))
chunks_final[0]

"""### **Modelo de Embeddings**

Para transformar cada fragmento en un vector num√©rico se seleccion√≥:

Modelo elegido: **sentence-transformers/paraphrase-multilingual-mpnet-base-v2**

Justificaci√≥n:

- Es un modelo multiling√ºe optimizado para espa√±ol.
- Destacado en tareas de similitud sem√°ntica y sistemas RAG.
- Produce embeddings densos de 768 dimensiones con alta calidad sem√°ntica.
- Eficiente
- Permite comparar consultas y documentos en el mismo espacio vectorial.
"""

!pip install sentence-transformers chromadb

from sentence_transformers import SentenceTransformer

EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

documents = [c["content"] for c in chunks_final]
metadatas = [c["metadata"] for c in chunks_final]
ids       = [c["id"]       for c in chunks_final]

embeddings = embedding_model.encode(
    documents,
    batch_size=64,
    show_progress_bar=True
)

"""### **Almacenamiento en ChromaDB**

Se utiliz√≥ ChromaDB con almacenamiento persistente, configurado con distancia coseno
"""

!pip install chromadb

import chromadb
from chromadb.config import Settings

chroma_client = chromadb.PersistentClient(path="chroma_vector_db")

# Creamos una nueva colecci√≥n
collection = chroma_client.get_or_create_collection(
    name="electrodomesticos_vector_store",
    metadata={"hnsw:space": "cosine"}
)

# Debido a una limitaci√≥n interna del cliente Rust, la colecci√≥n se carga en dos batch:
n = len(documents)
MAX_BATCH = 5461

print(f"Total de documentos a insertar: {n}")

print("Cargando primer batch...")
collection.add(
    documents=documents[:MAX_BATCH],
    metadatas=metadatas[:MAX_BATCH],
    ids=ids[:MAX_BATCH],
    embeddings=embeddings[:MAX_BATCH]
)

if n > MAX_BATCH:
    print("Cargando segundo batch...")
    collection.add(
        documents=documents[MAX_BATCH:],
        metadatas=metadatas[MAX_BATCH:],
        ids=ids[MAX_BATCH:],
        embeddings=embeddings[MAX_BATCH:]
    )

print("Carga finalizada.")
print("Documentos totales en Chroma:", collection.count())

"""**Funci√≥n de B√∫squeda Vectorial**

Esta funci√≥n es el m√©todo que utiliza el sistema para recuperar fragmentos relevantes.
Incluye:
- Normalizaci√≥n de filtros ($and autom√°tico)
- Manejo de errores
- Ordenamiento por relevancia
- Soporte opcional para embeddings de los documentos (√∫til para reranking)
"""

def normalizar_filtros(filtros):
    """
    Normaliza filtros para ChromaDB.
    Si el usuario pasa un dict simple como {"type": "manual", "producto":"X"},
    lo convierte autom√°ticamente a {"$and": [{"type":"manual"}, {"producto":"X"}]}
    """
    if filtros is None:
        return None

    # Si ya viene con operador ($and / $or / $not), lo dejamos como est√°
    if any(key.startswith("$") for key in filtros.keys()):
        return filtros

    # Convertimos dict simple ‚Üí $and
    condiciones = [{k: v} for k, v in filtros.items()]
    return {"$and": condiciones}


def buscar_vectorial(consulta, k=5, filtros=None, devolver_embeddings=False):
    """
    Realiza b√∫squeda vectorial en ChromaDB.

    Par√°metros:
        consulta (str): texto de la consulta del usuario
        k (int): cantidad de resultados a recuperar
        filtros (dict): condiciones para filtrar metadata
        devolver_embeddings (bool): si se desea incluir embeddings del resultado
                                     (√∫til para rerankers tipo CrossEncoder)

    Devuelve:
        lista de dicts: cada uno con id, contenido, metadata y distancia
    """
    # 1. Embedding de la consulta
    query_emb = embedding_model.encode([consulta])[0]

    # 2. Normalizar filtros a formato aceptado por Chroma
    where = normalizar_filtros(filtros)

    # 3. Ejecutar consulta en Chroma
    if where:
        result = collection.query(
            query_embeddings=[query_emb],
            n_results=k,
            where=where
        )
    else:
        result = collection.query(
            query_embeddings=[query_emb],
            n_results=k
        )

    # 4. Si no encuentra nada
    if len(result["ids"][0]) == 0:
        return []

    # 5. Construir resultados limpios
    resultados = []
    for i in range(len(result["ids"][0])):
        entrada = {
            "id": result["ids"][0][i],
            "content": result["documents"][0][i],
            "metadata": result["metadatas"][0][i],
            "distance": result["distances"][0][i]
        }

        if devolver_embeddings:
            entrada["embedding_doc"] = result["embeddings"][0][i]

        resultados.append(entrada)

    # 6. Ordenar por distancia (menor = m√°s relevante)
    resultados.sort(key=lambda x: x["distance"])

    return resultados

prueba = buscar_vectorial(
    "¬øC√≥mo usar la licuadora para hacer un smoothie?",
    k=3,
    filtros={"type": "manual", "producto_nombre": "Compacto Licuadora"}
)

for r in prueba:
    print(r["metadata"])
    print(r["content"][:200], "...\n---")

def responder_desde_vectorial_con_llm(query_usuario: str, fragmentos: list[dict], client):
    """
    Toma la consulta del usuario + los fragmentos recuperados de Chroma
    y genera una respuesta en lenguaje natural usando el LLM.
    """
    if not fragmentos:
        contexto = "No se encontraron fragmentos relevantes."
    else:
        partes = []
        for i, frag in enumerate(fragmentos):
            meta = frag.get("metadata", {})
            fuente = meta.get("fuente", "desconocida")
            partes.append(
                f"[{i+1}] (fuente: {fuente})\n{frag['content']}"
            )
        contexto = "\n\n".join(partes)

    prompt = f"""
Sos un asistente de soporte de una empresa de electrodom√©sticos.

Us√° EXCLUSIVAMENTE la siguiente informaci√≥n recuperada (fragmentos de manuales, rese√±as, FAQs o tickets de soporte)
para responder la pregunta del usuario.

Si la informaci√≥n no alcanza para responder con seguridad, dec√≠ claramente que no hay suficiente informaci√≥n
y suger√≠ reformular la pregunta.

Pregunta del usuario:
{query_usuario}

Fragmentos recuperados:
{contexto}

Respuesta (en espa√±ol, clara y directa):
"""

    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    return response.text.strip()

"""## **Base Tabular** (datos estad√≠sticos)
Para resolver consultas donde el usuario necesita informaci√≥n estructurada.

Las tablas provienen de los archivos CSV del dataset:
- productos.csv
- ventas_historicas.csv
- inventario_sucursales.csv
- devoluciones.csv
- vendedores.csv
- tickets_soporte.csv

####  Agrupamos las tablas que vamos a usar como fuente tabular
"""

tablas_tabulares = {
    "productos": productos,
    "inventario": inventario,
    "ventas": ventas,
    "devoluciones": devoluciones,
    "vendedores": vendedores,
    "tickets": tickets,
}

{nombre: df.shape for nombre, df in tablas_tabulares.items()}

"""### Generaci√≥n de estructura de cada DataFrame para modelo"""

def generar_estructura_para_llm(nombre_df: str, df: pd.DataFrame, max_uniques: int = 20):
    """
    Genera una estructura compacta con informaci√≥n relevante de la tabla,
    pensada para ser convertida a string y enviada a un LLM.

    Incluye:
    - nombre de la tabla
    - columnas
    - tipos de dato
    - cantidad de filas / columnas
    - cantidad de nulos por columna
    - resumen por columna:
        - num√©ricas: min / max
        - categ√≥ricas: valores √∫nicos (solo si son pocos)
        - texto: solo se marca como texto
    - estad√≠sticas num√©ricas generales (describe), por si se quiere usar aparte
    """

    estructura = {
        "nombre_df": nombre_df,
        "cantidad_filas": int(len(df)),
        "cantidad_columnas": int(df.shape[1]),
        "columnas": df.columns.tolist(),
        "tipos_de_dato": df.dtypes.astype(str).to_dict(),
        "valores_nulos": df.isnull().sum().astype(int).to_dict(),
        "resumen_columnas": {},
        "estadisticas_numericas": {}
    }

    # Resumen por columna
    for col in df.columns:
        serie = df[col]
        col_info = {}

        if pd.api.types.is_numeric_dtype(serie):
            col_info["tipo_logico"] = "numerico"
            col_info["min"] = float(serie.min()) if not serie.isna().all() else None
            col_info["max"] = float(serie.max()) if not serie.isna().all() else None

        elif pd.api.types.is_object_dtype(serie) or pd.api.types.is_categorical_dtype(serie):
            # Columna categ√≥rica / texto
            n_unique = serie.nunique(dropna=True)
            col_info["tipo_logico"] = "categorico" if n_unique <= max_uniques else "texto_largo"
            col_info["num_valores_unicos"] = int(n_unique)

            if n_unique <= max_uniques:
                col_info["valores_unicos"] = serie.dropna().unique().tolist()

        else:
            col_info["tipo_logico"] = "otro"

        estructura["resumen_columnas"][col] = col_info

    # Estad√≠sticas num√©ricas (describe)
    if not df.select_dtypes(include="number").empty:
        estructura["estadisticas_numericas"] = (
            df.select_dtypes(include="number").describe().to_dict()
        )

    return estructura

"""### Convertir la estructura a texto legible para el LLM"""

def estructura_a_string(estructura: dict) -> str:
    """
    Convierte la estructura generada por generar_estructura_para_llm
    a un string legible para usar en el prompt del LLM.
    """
    lineas = []
    lineas.append(f"Nombre de la tabla: {estructura['nombre_df']}")
    lineas.append(f"Filas: {estructura['cantidad_filas']}, Columnas: {estructura['cantidad_columnas']}")
    lineas.append("Columnas:")

    for col, info in estructura["resumen_columnas"].items():
        tipo = info.get("tipo_logico", "desconocido")

        if tipo == "numerico":
            lineas.append(
                f"- {col}: num√©rico, rango aproximado [{info.get('min')}, {info.get('max')}]"
            )
        elif tipo == "categorico":
            vals = ", ".join(map(str, info.get("valores_unicos", [])))
            lineas.append(
                f"- {col}: categ√≥rico, valores posibles: {vals}"
            )
        elif tipo == "texto_largo":
            lineas.append(
                f"- {col}: texto libre / muchas categor√≠as (‚âà{info.get('num_valores_unicos')} valores distintos)"
            )
        else:
            lineas.append(f"- {col}: tipo {tipo}")

    return "\n".join(lineas)

"""### Esquemas que se le pasan al LLM"""

import json

# 1) Estructuras en diccionarios
esquemas_tabulares = {
    nombre: generar_estructura_para_llm(nombre, df)
    for nombre, df in tablas_tabulares.items()
}

# 2) Versi√≥n legible como texto para meter en el prompt del LLM
descripciones_tablas = {
    nombre: estructura_a_string(estructura)
    for nombre, estructura in esquemas_tabulares.items()
}

"""### Generaci√≥n de prompt y conexi√≥n con gemini"""

from google.colab import userdata
userdata.get('GOOGLE_API_KEY')

# Inicializamos el cliente Gemini
from google import genai
from google.colab import userdata

API_KEY = userdata.get('GOOGLE_API_KEY')
client = genai.Client(api_key=API_KEY)

def armar_prompt_tabular(query_usuario: str) -> str:
    """
    Construye el prompt que se le va a enviar al LLM para que
    genere c√≥digo de pandas que responda a la consulta del usuario.
    """

    partes = []

    partes.append(
        "Sos un asistente experto en an√°lisis de datos con pandas.\n"
        "Trabaj√°s con datos de una empresa de electrodom√©sticos.\n\n"
        "Tu tarea es: dado una pregunta del usuario, responder EXCLUSIVAMENTE con c√≥digo Python\n"
        "que use DataFrames de pandas ya cargados para obtener la respuesta.\n\n"
        "IMPORTANTE:\n"
        "- NO crees DataFrames nuevos desde cero con datos escritos a mano.\n"
        "- NO uses read_csv ni lecturas de archivos.\n"
        "- NO imprimas nada, NO uses print.\n"
        "- NO expliques el c√≥digo, solo devolv√© la expresi√≥n de pandas.\n"
        "- La expresi√≥n debe devolver directamente el resultado (DataFrame o valor escalar).\n"
        "- Los DataFrames disponibles son:\n"
        "  * productos\n"
        "  * inventario\n"
        "  * ventas\n"
        "  * devoluciones\n"
        "  * vendedores\n"
        "  * tickets\n"
    )

    partes.append("\nA continuaci√≥n te doy un resumen de las tablas disponibles:\n")

    for nombre, desc in descripciones_tablas.items():
        partes.append(f"\n### Tabla: {nombre}\n{desc}\n")

    partes.append("\nPregunta del usuario:\n")
    partes.append(query_usuario)
    partes.append(
        "\n\nAhora respond√© √öNICAMENTE con c√≥digo Python v√°lido que utilice pandas "
        "y estos DataFrames para obtener la respuesta. No agregues explicaciones ni comentarios."
    )

    return "\n".join(partes)

def ejecutar_consulta_tabular(query_usuario: str):
    """
    Usa el LLM (Gemini) para generar c√≥digo de pandas a partir de la pregunta
    del usuario y luego ejecuta ese c√≥digo sobre los DataFrames ya cargados.

    Devuelve:
        - resultado: lo que devuelve la expresi√≥n de pandas (DataFrame, serie, escalar, etc.)
        - codigo_generado: el string de c√≥digo que gener√≥ el modelo
    """

    # 1) Armar el prompt a partir de tu funci√≥n
    prompt = armar_prompt_tabular(query_usuario)

    # 2) Llamar al modelo
    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    # 3) Extraer el texto y limpiar bloque de c√≥digo
    codigo_generado = response.text.strip()
    codigo_generado = (
        codigo_generado
        .replace("```python", "")
        .replace("```py", "")
        .replace("```", "")
        .strip()
    )

    print("üîß C√≥digo generado por el modelo:\n")
    print(codigo_generado)
    print("\n---\n")

    # 4) Ejecutar el c√≥digo. Necesita que los DataFrames est√©n en el scope global.
    try:
        resultado = eval(codigo_generado, globals())
    except Exception as e:
        print("‚ö†Ô∏è Error al ejecutar el c√≥digo generado:")
        print(e)
        resultado = None

    return resultado, codigo_generado

def responder_desde_dataframe_con_llm(query_usuario, resultado, client, modelo_gemini):
    """
    Toma la pregunta del usuario + resultado pandas (DataFrame, Serie o escalar)
    y genera una respuesta clara y natural usando el LLM.
    """

    import pandas as pd

    if isinstance(resultado, pd.DataFrame):
        if resultado.empty:
            tabla = "Tabla vac√≠a (sin resultados)."
        else:
            tabla = resultado.to_string(index=False)

    elif isinstance(resultado, pd.Series):
        tabla = resultado.to_string()

    else:
        tabla = str(resultado)

    prompt = f"""
Sos un asistente experto en an√°lisis de datos tabulares.

Te doy:
- La pregunta original del usuario.
- El resultado ya filtrado desde las tablas de electrodom√©sticos.

Tu tarea:
- Responder en espa√±ol, claro y directo.
- Explicar qu√© significa el resultado.
- NO mencionar pandas, ni c√≥digo, ni c√≥mo se obtuvo.
- Si est√° vac√≠o, sugerir reformular la consulta.

Pregunta del usuario:
{query_usuario}

Resultado:
{tabla}

Respuesta:
"""

    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    return response.text.strip()

def pipeline_tabular(query_usuario: str):
    """
    Pipeline completo para la base TABULAR:
    - Genera c√≥digo de pandas con el LLM
    - Ejecuta la expresi√≥n sobre los DataFrames
    - Genera una respuesta explicada en lenguaje natural
    """

    resultado, codigo = ejecutar_consulta_tabular(query_usuario)

    if resultado is None:
        return {
            "fuente": "tabular",
            "codigo_generado": codigo,
            "resultado_bruto": None,
            "respuesta": (
                "No pude ejecutar la consulta tabular. "
                "Prob√° reformular la pregunta o revisar los campos mencionados."
            ),
        }

    respuesta_nl = responder_desde_dataframe_con_llm(query_usuario, resultado, client)

    return {
        "fuente": "tabular",
        "codigo_generado": codigo,
        "resultado_bruto": resultado,
        "respuesta": respuesta_nl,
    }

resultado, codigo = ejecutar_consulta_tabular("Mostrame el top 10 de productos m√°s vendidos por cantidad en la tabla de ventas.")
print("Resultado:\n", resultado)

"""## **Base de datos de grafos**

### Extracci√≥n de relaciones de compatibilidad desde los manuales
"""

import re
import pandas as pd

compat_rows = []

for manual in manuales_docs:
    texto = manual["content"]
    producto_origen = manual.get("producto_id")
    nombre_origen = manual.get("producto_nombre")

    # Buscar secci√≥n "### Productos Compatibles ... (hasta la siguiente secci√≥n o el final)"
    patron_seccion = r"### Productos Compatibles(.*?)(## |$)"
    match = re.search(patron_seccion, texto, re.DOTALL)

    if not match:
        continue  # El manual no tiene secci√≥n de compatibilidad

    seccion = match.group(1)

    # Buscar l√≠neas de productos compatibles:
    # - **Nombre Producto** (`P0001`)
    patron_producto = r"- \*\*(.*?)\*\* .*?\(`(P\d+)`\)"
    productos_encontrados = re.findall(patron_producto, seccion)

    # Buscar l√≠neas con "Comparte: X"
    patron_comparte = r"Comparte:\s*([^\n\r]+)"
    comparte_list = re.findall(patron_comparte, seccion)

    # Emparejar producto destino con el texto de "comparte"
    for idx, (nombre_destino, id_destino) in enumerate(productos_encontrados):
        comparte = comparte_list[idx] if idx < len(comparte_list) else None

        compat_rows.append({
            "id_origen": producto_origen,
            "nombre_origen": nombre_origen,
            "id_destino": id_destino,
            "nombre_destino": nombre_destino,
            "comparte": comparte
        })

compat_df = pd.DataFrame(compat_rows)

print(compat_df.shape)
compat_df.head()

"""### Construcci√≥n de la tabla de productos para el grafo"""

# Productos que participan en alguna relaci√≥n de compatibilidad
ids_origen = compat_df["id_origen"]
ids_destino = compat_df["id_destino"]

ids_todos = pd.Series(
    pd.concat([ids_origen, ids_destino]).unique(),
    name="id_producto"
)

# Nos quedamos con columnas relevantes de la tabla productos
productos_grafo_df = (
    ids_todos.to_frame()
    .merge(
        productos[["id_producto", "nombre", "categoria", "marca"]],
        on="id_producto",
        how="left"
    )
)

print("Cantidad de productos en el grafo:", len(productos_grafo_df))
productos_grafo_df.head()

"""### Conexi√≥n a Neo4j y creaci√≥n del grafo de productos"""

!pip install py2neo

from google.colab import userdata
from py2neo import Graph, Node, Relationship

NEO4J_URI = "neo4j+s://1830c5fb.databases.neo4j.io"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = userdata.get("NEO4_KEY")

graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

# Test r√°pido de conexi√≥n
graph.run("RETURN 1 AS test").data()

"""### Carga de nodos :Producto en Neo4j"""

def cargar_productos_py2neo(graph, df):
    tx = graph.begin()

    for _, row in df.iterrows():
        nodo = Node(
            "Producto",
            id=row["id_producto"],
            nombre=row["nombre"],
            categoria=row["categoria"],
            marca=row["marca"]
        )
        tx.merge(nodo, "Producto", "id")  # upsert por id

    tx.commit()
    print(f"Se cargaron/actualizaron {len(df)} nodos :Producto.")

cargar_productos_py2neo(graph, productos_grafo_df)

"""### Carga de relaciones :COMPATIBLE_CON en Neo4j"""

def cargar_relaciones_py2neo(graph, df):
    tx = graph.begin()

    for _, row in df.iterrows():
        origen = graph.nodes.match("Producto", id=row["id_origen"]).first()
        destino = graph.nodes.match("Producto", id=row["id_destino"]).first()

        if origen and destino:
            rel = Relationship(origen, "COMPATIBLE_CON", destino)
            rel["comparte"] = row["comparte"]
            tx.merge(rel)

    tx.commit()
    print(f"Se cargaron/actualizaron {len(df)} relaciones :COMPATIBLE_CON.")

cargar_relaciones_py2neo(graph, compat_df)

"""#### Helper para ejecutar consultas Cypher desde Python"""

def ejecutar_cypher(query: str, params: dict | None = None):
    params = params or {}
    return graph.run(query, params).data()

"""### Prompt para que el LLM genere queries Cypher sobre el grafo"""

def armar_prompt_grafo(query_usuario: str) -> str:
    """
    Construye el prompt para que el LLM genere una query Cypher
    sobre el grafo de productos y compatibilidades.
    """

    return f"""
Sos un asistente experto en Neo4j y lenguaje Cypher.

TRABAJ√ÅS CON ESTE GRAFO:

Nodos:
  (:Producto)
    - id        (por ejemplo: "P0013")
    - nombre    (por ejemplo: "Procesadora")
    - categoria
    - marca

Relaciones:
  (:Producto)-[:COMPATIBLE_CON {{comparte: <string>}}]->(:Producto)

REGLAS ESTRICTAS PARA GENERAR CYPHER:

- SIEMPRE us√° un alias para la relaci√≥n:
      (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto)

- NUNCA pongas propiedades de la relaci√≥n dentro del MATCH.
   NO hacer cosas como:
      -[:COMPATIBLE_CON {{comparte: algo}}]->

- Las propiedades de la relaci√≥n se leen SOLO en el RETURN:
      r.comparte AS componente_compartido

- La query debe SIEMPRE devolver EXACTAMENTE estas columnas, en este orden:
      c.id        AS id_producto,
      c.nombre    AS nombre,
      c.categoria AS categoria,
      c.marca     AS marca,
      r.comparte  AS componente_compartido

- NO uses "null AS componente_compartido".
- NO agregues texto adicional, explicaciones ni comentarios.
- NO uses bloques ``` ni markdown.
- La consulta debe estar compuesta por UN SOLO statement Cypher.

El usuario puede preguntar en lenguaje natural por compatibilidades
entre productos (por nombre o por id). Us√° esa informaci√≥n para
construir el WHERE o el patr√≥n de MATCH correspondiente.

Pregunta del usuario:
{query_usuario}

Devolv√© √öNICAMENTE la query Cypher v√°lida que cumple todas las reglas anteriores.
"""

"""### Conversi√≥n de lenguaje natural a Cypher y ejecuci√≥n en Neo4j"""

def consulta_grafo_con_llm(query_usuario: str):
    """
    Usa el LLM para convertir una pregunta en lenguaje natural
    a una query Cypher, ejecuta esa query en Neo4j
    y devuelve (resultados, cypher_generado).
    """
    prompt = armar_prompt_grafo(query_usuario)

    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    cypher_generado = (
        response.text.replace("```cypher", "")
        .replace("```", "")
        .strip()
    )

    print("üîß Cypher generado por el modelo:\n")
    print(cypher_generado)
    print("\n---\n")

    try:
        resultados = ejecutar_cypher(cypher_generado)
    except Exception as e:
        print("‚ö†Ô∏è Error al ejecutar la query Cypher:")
        print(e)
        resultados = []

    return resultados, cypher_generado

"""### Generaci√≥n de respuesta en lenguaje natural desde el resultado del grafo"""

def responder_desde_grafo_con_llm(query_usuario: str, resultados: list[dict], client):
    """
    Toma la pregunta original + los resultados (lista de diccionarios)
    y le pide al LLM que genere una respuesta explicada.
    """
    if not resultados:
        tabla = "Sin resultados (la consulta al grafo no devolvi√≥ filas)."
    else:
        df_res = pd.DataFrame(resultados)
        tabla = df_res.to_string(index=False)

    prompt = f"""
Sos un asistente que trabaja con un grafo de productos de electrodom√©sticos.

Te doy la pregunta original del usuario y el resultado de una consulta a Neo4j.
El resultado ya contiene los productos compatibles (si los hay).

Tu tarea:
- Responder en espa√±ol, de forma clara y directa.
- Explicar qu√© productos son compatibles, si comparten componentes, etc.
- Si no hay resultados, explic√° que no se encontr√≥ compatibilidad para ese caso.
- NO expliques la query Cypher ni los detalles internos del grafo.

Pregunta del usuario:
{query_usuario}

Resultado de la consulta al grafo:
{tabla}

Respuesta:
"""

    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    return response.text.strip()

"""### Pipeline completo de la fuente "grafo"
"""

def pipeline_grafo(query_usuario: str):
    resultados, cypher = consulta_grafo_con_llm(query_usuario)
    respuesta_nl = responder_desde_grafo_con_llm(query_usuario, resultados, client)
    return {
        "fuente": "grafo",
        "cypher_generado": cypher,
        "resultado_bruto": resultados,
        "respuesta": respuesta_nl,
    }

#Ejemplo de prueba puntual:
# resp_g = pipeline_grafo("¬øQu√© productos son compatibles con la Procesadora?")
# print(resp_g["respuesta"])

"""## **Clasificador de intenci√≥n avanzado**

En esta secci√≥n se desarrolla un **clasificador de intenci√≥n** que decide a qu√© fuente de datos
debe dirigirse el sistema seg√∫n la pregunta del usuario:

- **Vectorial**: consultas t√≠picas de manuales, FAQs, rese√±as y problemas de uso de productos.
- **Tabular**: consultas sobre datos estructurados (ventas, stock, devoluciones, totales, rankings, etc.).
- **Grafo**: consultas sobre **compatibilidad** entre productos y repuestos.

Para cumplir con el enunciado se implementan dos clasificadores:

1. Un **clasificador entrenado propio**, basado en TF-IDF + Logistic Regression, utilizando un
   conjunto de preguntas sint√©ticas representativas de cada tipo de fuente.
2. Un **clasificador basado en LLM** (Gemini) con *few-shot prompting*, donde se le explican
   las clases al modelo y se le dan ejemplos de preguntas ya etiquetadas.

Finalmente, se comparan ambos clasificadores utilizando m√©tricas de clasificaci√≥n y se justifica
qu√© enfoque resulta m√°s adecuado para el sistema.

### Dataset de ejemplos sint√©ticos
"""

import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

import google.generativeai as genai
genai.configure(api_key="AIzaE")

MODELO_GEMINI_CLF = modelo_gemini

# Cada tupla es: (texto_pregunta, etiqueta_real)
# etiquetas: "vectorial", "tabular", "grafo"

datos_clasificacion = [
    # VECTORIAL: manuales, FAQs, problemas de uso, rese√±as
    ("¬øQu√© voltaje requiere la licuadora modelo P0001?", "vectorial"),
    ("¬øC√≥mo se limpia el filtro del lavarropas?", "vectorial"),
    ("¬øEste microondas tiene funci√≥n grill?", "vectorial"),
    ("Mi heladera hace un ruido raro, ¬øes normal?", "vectorial"),
    ("Mostrame las instrucciones para usar la procesadora", "vectorial"),
    ("¬øCu√°nto mide el cable de este ventilador?", "vectorial"),
    ("¬øC√≥mo reinicio el horno si se queda trabado?", "vectorial"),
    ("¬øQu√© garant√≠a tiene el modelo P0003?", "vectorial"),
    ("¬øD√≥nde est√°n las instrucciones de seguridad de la licuadora?", "vectorial"),
    ("¬øC√≥mo cambio el filtro de la aspiradora?", "vectorial"),

    # TABULAR: ventas, stock, precios, KPIs
    ("Mostrame las ventas totales del mes pasado", "tabular"),
    ("¬øCu√°ntas unidades de la heladera P0005 hay en stock?", "tabular"),
    ("List√° los vendedores con m√°s devoluciones", "tabular"),
    ("¬øQu√© producto tuvo m√°s tickets de soporte en 2024?", "tabular"),
    ("Dame el top 5 de productos m√°s vendidos", "tabular"),
    ("Mostrame el total facturado por categor√≠a este a√±o", "tabular"),
    ("¬øCu√°l fue la sucursal con m√°s ventas este trimestre?", "tabular"),
    ("Mostrame el stock actual de todos los lavarropas", "tabular"),
    ("¬øQu√© vendedor tuvo m√°s ventas de licuadoras?", "tabular"),
    ("¬øCu√°ntas devoluciones hubo de la procesadora P0010?", "tabular"),

    # GRAFO: compatibilidad entre productos/repuestos
    ("¬øQu√© repuestos son compatibles con la licuadora P0001?", "grafo"),
    ("¬øEste filtro es compatible con qu√© modelos de lavarropas?", "grafo"),
    ("List√° todos los productos compatibles con el horno P0100", "grafo"),
    ("¬øQu√© modelos comparten el mismo motor que la aspiradora P0200?", "grafo"),
    ("¬øCon qu√© otros productos es compatible la pieza X123?", "grafo"),
    ("Mostrame todos los productos que comparten repuestos con la licuadora P0003", "grafo"),
    ("¬øQu√© modelos usan el mismo filtro que la heladera P0300?", "grafo"),
    ("¬øLa resistencia R45 es compatible con qu√© hornos?", "grafo"),
    ("¬øQu√© otros modelos aceptan este mismo repuesto?", "grafo"),
    ("Mostrame todos los productos relacionados por compatibilidad con P0500", "grafo"),
]

df_clf = pd.DataFrame(datos_clasificacion, columns=["texto", "label"])

"""### Clasificador entrenado propio (TF-IDF + Logistic Regression)"""

# 1) Partici√≥n train / test
X = df_clf["texto"]
y = df_clf["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# 2) Vectorizador TF-IDF
vectorizer_intencion = TfidfVectorizer(
    ngram_range=(1, 2),   # unigrama + bigrama
    min_df=1
)

X_train_vec = vectorizer_intencion.fit_transform(X_train)
X_test_vec  = vectorizer_intencion.transform(X_test)

# 3) Modelo de clasificaci√≥n
clf_intencion = LogisticRegression(
    max_iter=200,
    multi_class="auto"
)
clf_intencion.fit(X_train_vec, y_train)

# 4) Evaluaci√≥n en test (modelo entrenado propio)
y_pred = clf_intencion.predict(X_test_vec)

print("=== CLASIFICADOR ENTRENADO (TF-IDF + LogisticRegression) ===")
print(classification_report(y_test, y_pred))
print("Matriz de confusi√≥n:")
print(confusion_matrix(y_test, y_pred))


def clasificar_intencion_ml(pregunta: str) -> str:
    """
    Clasificador de intenci√≥n basado en modelo entrenado (TF-IDF + LogisticRegression).

    Devuelve una de las etiquetas:
      - "vectorial"
      - "tabular"
      - "grafo"
    """
    X_vec = vectorizer_intencion.transform([pregunta])
    pred = clf_intencion.predict(X_vec)[0]
    return pred

"""Para el clasificador entrenado se construy√≥ un conjunto de **30 preguntas sint√©ticas**,
distribuidas equitativamente en las tres clases de intenci√≥n:

- `vectorial`
- `tabular`
- `grafo`

Cada pregunta representa un tipo de consulta que luego podr√≠a hacer un usuario real del sistema
(por ejemplo, preguntas sobre uso de productos, sobre ventas/stock o sobre compatibilidad de repuestos).

El pipeline utilizado fue:

1. Vectorizaci√≥n de las preguntas con **TF-IDF**, considerando unigramas y bigramas.
2. Divisi√≥n del dataset en entrenamiento y prueba mediante `train_test_split`, manteniendo el balance entre clases.
3. Entrenamiento de un modelo de **Logistic Regression**.
4. Evaluaci√≥n sobre el conjunto de prueba usando `classification_report` y `confusion_matrix`.

En el conjunto de prueba se obtuvieron m√©tricas cercanas a:

- **Accuracy** ‚âà 0.78  

Este clasificador tiene como ventaja principal su **bajo costo de c√≥mputo** y su independencia de servicios externos, por lo que es una buena opci√≥n cuando se prioriza eficiencia y simplicidad.

### Clasificador basado en LLM (Few-Shot) y comparaci√≥n
"""

def armar_prompt_clasificador_llm(pregunta: str) -> str:
    """
    Construye el prompt para que Gemini clasifique la intenci√≥n
    en una de las tres clases: vectorial / tabular / grafo.
    """

    return f"""
Sos un asistente que clasifica preguntas de usuarios de una empresa de electrodom√©sticos
en TRES categor√≠as seg√∫n la FUENTE DE DATOS que habr√≠a que usar.

Las clases posibles son:

1) vectorial:
   - Preguntas sobre c√≥mo usar un producto.
   - Consultas t√≠picas de manuales, FAQs, problemas de uso, rese√±as.
   - Ejemplos: instrucciones, voltaje, limpieza, garant√≠a, problemas de funcionamiento.

2) tabular:
   - Preguntas sobre datos num√©ricos o estad√≠sticos.
   - Ventas, stock, devoluciones, rankings, totales, top N, facturaci√≥n, por sucursal.
   - Ejemplos: "top 10 m√°s vendidos", "total facturado", "stock disponible", "vendedor con m√°s ventas".

3) grafo:
   - Preguntas sobre compatibilidad entre productos o repuestos.
   - Qu√© modelos comparten componentes, qu√© repuestos sirven para qu√© modelos.
   - Ejemplos: "¬øCon qu√© modelos es compatible?", "¬øqu√© productos comparten el mismo motor?".

EJEMPLOS (few-shot):

PREGUNTA: "¬øQu√© voltaje requiere la licuadora modelo P0001?"
CLASE: vectorial

PREGUNTA: "¬øC√≥mo se limpia el filtro del lavarropas?"
CLASE: vectorial

PREGUNTA: "Mostrame las ventas totales del mes pasado"
CLASE: tabular

PREGUNTA: "¬øCu√°ntas unidades de la heladera P0005 hay en stock?"
CLASE: tabular

PREGUNTA: "¬øQu√© repuestos son compatibles con la licuadora P0001?"
CLASE: grafo

PREGUNTA: "Mostrame todos los productos que comparten repuestos con la licuadora P0003"
CLASE: grafo


Ahora clasific√° la siguiente pregunta del usuario EN UNA SOLA PALABRA,
usando exactamente una de estas opciones:

- vectorial
- tabular
- grafo

No agregues explicaciones ni texto extra.

PREGUNTA A CLASIFICAR:
{pregunta}

RESPUESTA:
""".strip()


def clasificar_intencion_llm(pregunta: str) -> str:
    """
    Clasificador de intenci√≥n basado en LLM (Gemini) con few-shot prompting.

    Devuelve:
      - "vectorial"
      - "tabular"
      - "grafo"
    """

    prompt = armar_prompt_clasificador_llm(pregunta)

    response = client.models.generate_content(
        model=modelo_gemini,
        contents=[prompt]
    )

    texto = response.text.strip().lower()

    # Normalizamos por si el modelo devuelve algo tipo "La clase es: tabular"
    if "vectorial" in texto:
        return "vectorial"
    if "tabular" in texto:
        return "tabular"
    if "grafo" in texto:
        return "grafo"

    # Fallback: si no podemos parsear, usamos el modelo entrenado
    return clasificar_intencion_ml(pregunta)

"""### Comparaci√≥n de clasificadores"""

def comparar_clasificadores(df=df_clf, max_muestras_llm: int = 20):
    """
    Compara el clasificador entrenado (ML) y el clasificador LLM few-shot.

    - Para el modelo entrenado se calcula classification_report completo.
    - Para el LLM se eval√∫a sobre un subconjunto (max_muestras_llm) para
      evitar demasiadas llamadas a la API.

    Imprime las m√©tricas por pantalla.
    """

    # --- Modelo entrenado (ML) ya lo evaluamos arriba con X_test e y_test ---
    print("\n\n=== RESUMEN: CLASIFICADOR ENTRENADO (ML) ===")
    X_vec = vectorizer_intencion.transform(df["texto"])
    y_true = df["label"]
    y_pred_ml = clf_intencion.predict(X_vec)
    print(classification_report(y_true, y_pred_ml))

    # --- Clasificador LLM (few-shot) en un subconjunto ---
    print("\n=== RESUMEN: CLASIFICADOR LLM (FEW-SHOT) ===")
    # Subconjunto para no gastar demasiadas llamadas
    sub = df.sample(min(max_muestras_llm, len(df)), random_state=42)

    y_true_llm = []
    y_pred_llm = []

    for _, row in sub.iterrows():
        txt = row["texto"]
        etiqueta_real = row["label"]
        pred_llm = clasificar_intencion_llm(txt)

        y_true_llm.append(etiqueta_real)
        y_pred_llm.append(pred_llm)

        print(f"Pregunta: {txt}")
        print(f"  Real: {etiqueta_real} | LLM: {pred_llm}")
        print("---")

    print("\n=== M√âTRICAS LLM EN SUBCONJUNTO ===")
    print(classification_report(y_true_llm, y_pred_llm))

"""#### Funcion unificada para el asistente"""

def clasificar_intencion(query_usuario: str, metodo: str = "ml") -> str:
    """
    Envuelve ambos clasificadores y te deja elegir el m√©todo:

    metodo = "ml"   -> usa clasificador entrenado (TF-IDF + LogisticRegression)
    metodo = "llm"  -> usa clasificador basado en LLM few-shot
    metodo = "mixto"-> usa ML y, si quisiera, podr√≠a combinar (ac√° por ahora = ML)
    """

    metodo = metodo.lower()

    if metodo == "llm":
        return clasificar_intencion_llm(query_usuario)

    # En "mixto" podr√≠amos usar ML y LLM juntos; por ahora, usamos ML como default
    return clasificar_intencion_ml(query_usuario)

def asistente_electro(
    query_usuario: str,
    historial=None,
    k_vectorial: int = 5,
    metodo_clasificador: str = "ml"
):
    """
    Punto de entrada √∫nico del asistente.
    - Usa el clasificador de intenci√≥n (ML o LLM)
    - Llama al pipeline correspondiente (vectorial, tabular o grafo)
    """

    fuente = clasificar_intencion(query_usuario, metodo=metodo_clasificador)

    if fuente == "vectorial":
        # Si quer√©s usar la versi√≥n avanzada con h√≠brida + rerank:
        # return pipeline_vectorial(query_usuario, k_final=k_vectorial)

        # Si prefer√≠s algo m√°s simple por ahora:
        fragmentos = buscar_vectorial(query_usuario, k=k_vectorial)
        respuesta = responder_desde_vectorial_con_llm(query_usuario, fragmentos, client)
        return {
            "fuente": "vectorial",
            "fragmentos": fragmentos,
            "respuesta": respuesta,
        }

    elif fuente == "tabular":
        return pipeline_tabular(query_usuario)

    elif fuente == "grafo":
        return pipeline_grafo(query_usuario)

    # Fallback raro, por las dudas
    return {
        "fuente": "desconocida",
        "respuesta": (
            "Por ahora no s√© a qu√© fuente de datos dirigir esta consulta. "
            "Prob√° reformular la pregunta con m√°s detalle sobre si busc√°s precios, "
            "compatibilidades o instrucciones de uso."
        )
    }

# Ejemplo de uso del asistente integrado

resp = asistente_electro("¬øQu√© productos son compatibles con la Procesadora?")
print(resp["fuente"])
print()
print(resp["respuesta"])

comparar_clasificadores()

"""Adem√°s del modelo entrenado tradicional, se implement√≥ un **clasificador de intenci√≥n basado en LLM**
utilizando Gemini. Para esto se dise√±√≥ un prompt que:

- Describe en lenguaje natural las tres clases posibles (`vectorial`, `tabular`, `grafo`).
- Incluye **ejemplos etiquetados** de preguntas t√≠picas de cada clase (enfoque *few-shot*).
- Le indica expl√≠citamente al modelo que debe responder **solo con el nombre de la clase**.

A partir de este prompt, la funci√≥n `clasificar_intencion_llm` toma una pregunta en lenguaje natural,
consulta al modelo y normaliza su salida para obtener una de las tres etiquetas.

Para comparar ambos enfoques se utiliz√≥ la funci√≥n `comparar_clasificadores`, que eval√∫a:

- El **clasificador entrenado (ML)** sobre todo el dataset sint√©tico.
- El **clasificador LLM** sobre un subconjunto de ejemplos, para evitar un n√∫mero excesivo de llamadas a la API.

En los experimentos realizados se observaron resultados del estilo:

- Clasificador entrenado (ML):  
  - accuracy ‚âà 0.93  
  - buen F1-score en las tres clases

- Clasificador LLM (few-shot):  
  - accuracy ‚âà 1.00 en el subconjunto evaluado  
  - clasificaci√≥n correcta en todos los ejemplos probados

Estos resultados muestran que el LLM, guiado con ejemplos adecuados, es capaz de captar muy bien
la diferencia sem√°ntica entre las tres intenciones. Sin embargo, su uso implica:

- **Mayor latencia** (cada predicci√≥n requiere una llamada a la API).
- **Costo asociado** al uso del modelo en la nube.
- Dependencia de la disponibilidad del servicio externo.

Por otro lado, el clasificador entrenado con TF-IDF + Logistic Regression ofrece:

- Muy buen rendimiento (m√°s del 90% de accuracy).
- Costo de c√≥mputo despreciable una vez entrenado.
- Independencia de APIs externas.

Por este motivo, en la implementaci√≥n final del asistente se ofrece la posibilidad de usar
ambos clasificadores a trav√©s del par√°metro `metodo_clasificador`, pero se deja como opci√≥n
por defecto el **clasificador entrenado (ML)**, priorizando eficiencia y simplicidad en el
entorno de producci√≥n, y manteniendo el clasificador LLM como alternativa m√°s ‚Äúinteligente‚Äù
pero tambi√©n m√°s costosa.

## Recuperaci√≥n H√≠brida (BM25 + Embeddings)

Para mejorar la calidad de la recuperaci√≥n en la fuente vectorial, se implementa un
pipeline h√≠brido que combina dos enfoques complementarios:

1. **B√∫squeda por palabras clave (BM25)**  
   Permite detectar coincidencias literales y t√©rminos espec√≠ficos, siendo muy √∫til cuando
   el usuario menciona palabras exactas del manual o la FAQ.

2. **B√∫squeda sem√°ntica mediante embeddings**  
   Utiliza el espacio vectorial generado previamente, detectando similitud sem√°ntica
   incluso cuando no se repiten las mismas palabras.

La combinaci√≥n de ambos m√©todos permite cubrir casos donde:

- El usuario recuerda parte de una frase del manual (BM25 funciona bien).
- El usuario parafrasea o hace una pregunta conceptual no textual (embeddings funcionan mejor).
"""

# Corpus de texto para BM25 (uno por chunk)
bm25_corpus = [c["content"] for c in chunks_final]

# Mapeo √≠ndice entero -> chunk original
bm25_id_to_chunk = chunks_final  # lista, √≠ndice = entero usado por BM25

!pip install txtai

"""### B√∫squeda por palabras clave con BM25

Se construy√≥ un √≠ndice BM25 utilizando la librer√≠a `txtai`, donde cada documento
representa un *chunk* de los manuales, rese√±as y FAQs.

El objetivo de esta etapa es recuperar r√°pidamente candidatos que contienen t√©rminos
mencionados por el usuario. La funci√≥n `buscar_bm25()` recibe una consulta y devuelve
los `k` chunks m√°s relevantes seg√∫n BM25.

BM25 es especialmente √∫til cuando el usuario utiliza t√©rminos espec√≠ficos del manual o
palabras que aparecen textualmente en los documentos.
"""

from txtai.scoring import ScoringFactory

# √çndice BM25
bm25_scoring = ScoringFactory.create({
    "method": "bm25",
    "terms": True  # √≠ndice de palabras clave
})

# Indexar todos los chunks
bm25_scoring.index(
    ( (idx, text, None) for idx, text in enumerate(bm25_corpus) )
)

print("√çndice BM25 construido. Cantidad de documentos:", bm25_scoring.count())

def buscar_bm25(consulta: str, k: int = 10):
    """
    B√∫squeda por palabras clave usando BM25 sobre los chunks.
    Devuelve una lista de dicts con:
    - id, content, metadata, score_bm25
    """
    resultados_raw = bm25_scoring.search(consulta, k)

    resultados = []
    for idx, score in resultados_raw:
        chunk = bm25_id_to_chunk[idx]
        resultados.append({
            "id": chunk["id"],
            "content": chunk["content"],
            "metadata": chunk["metadata"],
            "score_bm25": float(score),
        })

    return resultados

import math

def _normalizar_scores(valores):
    """
    Normaliza una lista de valores a [0, 1].
    Si todos son iguales, devuelve 1.0 para todos.
    """
    if not valores:
        return []

    vmin = min(valores)
    vmax = max(valores)

    if math.isclose(vmin, vmax):
        return [1.0 for _ in valores]

    return [(v - vmin) / (vmax - vmin) for v in valores]

"""### B√∫squeda H√≠brida: combinaci√≥n BM25 + b√∫squeda sem√°ntica

La funci√≥n `buscar_hibrida()` fusiona los resultados de BM25 y la b√∫squeda vectorial
sem√°ntica. Para cada chunk candidato se calculan dos puntajes:

- `score_bm25_norm`: puntaje normalizado de BM25
- `score_sem_norm`: puntaje normalizado de la similitud sem√°ntica

Luego se combinan mediante un hiperpar√°metro `alpha`.

Este mecanismo garantiza que:

- Si el usuario usa palabras exactas ‚Üí BM25 aporta mayor peso.
- Si el usuario expresa una idea m√°s conceptual ‚Üí embeddings compensan mejor.

Finalmente se ordenan los chunks seg√∫n el score h√≠brido y se devuelven los mejores candidatos.
"""

def buscar_hibrida(
    consulta: str,
    k_bm25: int = 20,
    k_sem: int = 20,
    k_final: int = 10,
    alpha: float = 0.5,
    filtros: dict | None = None,
    devolver_embeddings: bool = False,
):
    """
    B√∫squeda h√≠brida que combina:
    - BM25 (palabras clave)
    - B√∫squeda vectorial (embeddings en Chroma)

    alpha controla el peso de BM25:
      score_hibrido = alpha * score_bm25_norm + (1 - alpha) * score_sem_norm
    """

    # 1) BM25
    res_bm25 = buscar_bm25(consulta, k=k_bm25)

    # 2) B√∫squeda vectorial (ya existente)
    res_vec = buscar_vectorial(
        consulta,
        k=k_sem,
        filtros=filtros,
        devolver_embeddings=devolver_embeddings
    )

    # 3) Armar diccionario combinado por id de chunk
    combinado = {}

    # BM25
    for r in res_bm25:
        cid = r["id"]
        combinado.setdefault(cid, {
            "id": cid,
            "content": r["content"],
            "metadata": r["metadata"],
            "score_bm25": None,
            "score_sem": None,
            "score_hibrido": None,
        })
        combinado[cid]["score_bm25"] = r["score_bm25"]

    # Vectorial: usamos -distance como ‚Äúscore crudo‚Äù (menor distancia = mejor ‚Üí mayor score)
    for r in res_vec:
        cid = r["id"]
        combinado.setdefault(cid, {
            "id": cid,
            "content": r["content"],
            "metadata": r["metadata"],
            "score_bm25": None,
            "score_sem": None,
            "score_hibrido": None,
        })
        # guardamos tambi√©n por si quer√©s usar distance directo
        combinado[cid]["distance"] = r["distance"]
        combinado[cid]["score_sem_raw"] = -float(r["distance"])
        if devolver_embeddings and "embedding_doc" in r:
            combinado[cid]["embedding_doc"] = r["embedding_doc"]

    # 4) Normalizar scores
    # BM25
    scores_bm25 = [
        v["score_bm25"]
        for v in combinado.values()
        if v["score_bm25"] is not None
    ]
    scores_sem_raw = [
        v["score_sem_raw"]
        for v in combinado.values()
        if "score_sem_raw" in v
    ]

    scores_bm25_norm = _normalizar_scores(scores_bm25)
    scores_sem_norm = _normalizar_scores(scores_sem_raw)

    # Mapear normalizados de vuelta a las entradas
    # (hacemos listas paralelas para simplificar)
    i_bm25 = 0
    for v in combinado.values():
        if v["score_bm25"] is not None:
            v["score_bm25_norm"] = scores_bm25_norm[i_bm25]
            i_bm25 += 1
        else:
            v["score_bm25_norm"] = 0.0

    i_sem = 0
    for v in combinado.values():
        if "score_sem_raw" in v:
            v["score_sem_norm"] = scores_sem_norm[i_sem]
            i_sem += 1
        else:
            v["score_sem_norm"] = 0.0

    # 5) Score h√≠brido
    for v in combinado.values():
        sb = v.get("score_bm25_norm", 0.0)
        ss = v.get("score_sem_norm", 0.0)
        v["score_hibrido"] = alpha * sb + (1 - alpha) * ss

    # 6) Ordenar y devolver top-k final
    lista = list(combinado.values())
    lista.sort(key=lambda x: x["score_hibrido"], reverse=True)

    return lista[:k_final]

"""## Re-Ranking con CrossEncoder

Luego de obtener los candidatos h√≠bridos, se aplica una etapa de **re-ranking** utilizando
un modelo CrossEncoder (`ms-marco-MiniLM-L-2-v2`).  
A diferencia de los embeddings, el CrossEncoder analiza la consulta y el documento juntos,
permitiendo evaluar relaciones m√°s profundas entre ambos.

El proceso consiste en:

1. Tomar los candidatos producidos por la b√∫squeda h√≠brida.
2. Evaluar cada par *(consulta, chunk)* con el CrossEncoder.
3. Ordenarlos nuevamente seg√∫n este puntaje m√°s preciso.

Este re-ranking mejora fuertemente la calidad de los primeros resultados (top-k),
siguiendo las recomendaciones est√°ndar en sistemas RAG profesionales.
"""

from txtai.pipeline import Similarity

# Modelo de CrossEncoder para rerank
crossencoder = Similarity(
    "cross-encoder/ms-marco-MiniLM-L-2-v2",
    crossencode=True,
    gpu=True
)

def rerank_crossencoder(consulta: str, resultados: list[dict], top_k: int = 5):
    """
    Reordena los resultados usando un modelo cross-encoder.
    Toma:
      - consulta (texto)
      - resultados: lista de dicts con al menos 'content'
      - top_k: cu√°ntos devolver

    Devuelve:
      - lista de dicts con campo extra 'score_rerank'
    """
    if not resultados:
        return []

    textos = [r["content"] for r in resultados]

    # Similarity devuelve lista de (id, score), ordenada por score desc
    pares = crossencoder(consulta, textos)

    rerankeados = []
    for idx, score in pares[:top_k]:
        item = resultados[idx].copy()
        item["score_rerank"] = float(score)
        rerankeados.append(item)

    return rerankeados

"""## Pipeline Vectorial Avanzado

Se define un pipeline completo para la fuente VECTORIAL:

1. **B√∫squeda h√≠brida (BM25 + embeddings)**
2. **Re-ranking mediante CrossEncoder**
3. **Construcci√≥n de la respuesta final con un LLM**, usando como contexto solo los
   `k` chunks mejor posicionados.

Esta arquitectura representa un pipeline RAG consistente con sistemas de producci√≥n,
donde se combinan t√©cnicas de recuperaci√≥n lexical, recuperaci√≥n sem√°ntica y
re-ranking profundo.

El resultado es un sistema robusto, capaz de recuperar informaci√≥n precisa incluso
cuando el usuario formula preguntas incompletas, vagas o parafraseadas.

"""

def pipeline_vectorial(
    query_usuario: str,
    k_bm25: int = 20,
    k_sem: int = 20,
    k_final: int = 5,
    alpha: float = 0.5,
    filtros: dict | None = None,
):
    """
    Pipeline completo para la fuente VECTORIAL:
    1) Recupera candidatos con b√∫squeda h√≠brida (BM25 + embeddings).
    2) Re-rankea los candidatos con un CrossEncoder.
    3) Genera respuesta en lenguaje natural con el LLM.
    """

    # 1) B√∫squeda h√≠brida (BM25 + vectorial)
    candidatos = buscar_hibrida(
        consulta=query_usuario,
        k_bm25=k_bm25,
        k_sem=k_sem,
        k_final=max(k_final * 2, 10),  # recupero un poco m√°s para rerank
        alpha=alpha,
        filtros=filtros,
        devolver_embeddings=False
    )

    # 2) Re-rank con CrossEncoder
    rerankeados = rerank_crossencoder(
        consulta=query_usuario,
        resultados=candidatos,
        top_k=k_final
    )

    # Si por alg√∫n motivo el rerank falla o da vac√≠o, usamos los candidatos originales
    fragmentos_finales = rerankeados if rerankeados else candidatos[:k_final]

    # 3) Respuesta con LLM usando estos fragmentos
    respuesta = responder_desde_vectorial_con_llm(
        query_usuario,
        fragmentos_finales,
        client
    )

    return {
        "fuente": "vectorial",
        "fragmentos": fragmentos_finales,
        "respuesta": respuesta,
    }

"""## Asistente integrado con b√∫squeda h√≠brida y ReRank

A partir de los componentes anteriores (BM25, b√∫squeda sem√°ntica, b√∫squeda h√≠brida
y re-ranqueo con CrossEncoder), se define una versi√≥n avanzada del asistente que
integra:

- Clasificador de intenci√≥n (ML o LLM)
- Pipeline h√≠brido para la fuente vectorial
- Pipelines din√°micos para datos tabulares y grafo
"""

def asistente_electro_avanzado(
    query_usuario: str,
    historial=None,
    k_vectorial: int = 5,
    metodo_clasificador: str = "ml"
):
    """
    Versi√≥n AVANZADA del asistente.

    Diferencias con asistente_electro (b√°sico):
    - Para la fuente VECTORIAL usa el pipeline h√≠brido + rerank:
        pipeline_vectorial(...)
    - Para TABULAR y GRAFO reutiliza los mismos pipelines din√°micos.

    El clasificador de intenci√≥n puede ser:
    - "ml"  -> clasificador entrenado TF-IDF + LogisticRegression
    - "llm" -> clasificador basado en Gemini few-shot
    """

    # 1) Clasificar la intenci√≥n de la consulta
    fuente = clasificar_intencion(query_usuario, metodo=metodo_clasificador)

    # 2) Elegir pipeline seg√∫n la fuente
    if fuente == "vectorial":
        # Ahora s√≠ usamos la b√∫squeda h√≠brida + rerank + RAG
        resultado = pipeline_vectorial(
            query_usuario,
            k_bm25=20,
            k_sem=20,
            k_final=k_vectorial,
            alpha=0.5,
            filtros=None,
        )
        return resultado

    elif fuente == "tabular":
        return pipeline_tabular(query_usuario)

    elif fuente == "grafo":
        return pipeline_grafo(query_usuario)

    # Fallback por las dudas
    return {
        "fuente": "desconocida",
        "respuesta": (
            "No pude determinar a qu√© fuente de datos dirigir esta consulta. "
            "Prob√° reformular la pregunta indicando si busc√°s datos num√©ricos, "
            "instrucciones de uso o compatibilidad entre productos."
        )
    }

# Versi√≥n b√°sica del asistente (usa b√∫squeda vectorial simple)
resp_basico = asistente_electro(
    "¬øQu√© voltaje requiere la licuadora modelo P0001?",
    metodo_clasificador="ml"
)
print("Asistente b√°sico ‚Üí fuente:", resp_basico["fuente"])
print(resp_basico["respuesta"])

print("\n" + "="*60 + "\n")

# Versi√≥n avanzada del asistente (usa b√∫squeda h√≠brida + rerank para vectorial)
resp_avanzado = asistente_electro_avanzado(
    "¬øQu√© voltaje requiere la licuadora modelo P0001?",
    metodo_clasificador="ml"
)
print("Asistente avanzado ‚Üí fuente:", resp_avanzado["fuente"])
print(resp_avanzado["respuesta"])

"""## Elecci√≥n del LLM y Justificaci√≥n del Entorno de Ejecuci√≥n

Para este Trabajo Pr√°ctico se utiliza un modelo de lenguaje alojado **en la nube**, espec√≠ficamente
el modelo **Gemini** de Google. La elecci√≥n de un modelo en la nube se justifica por los
siguientes motivos:

1. **Limitaciones computacionales locales:**  
   Los modelos LLM modernos requieren GPU potentes y memoria significativa.
   Ejecutarlos localmente en un entorno como Google Colab o notebooks personales  
   no es viable sin hardware especializado.

2. **Disponibilidad inmediata y sin configuraci√≥n:**  
   El acceso a Gemini a trav√©s de API permite utilizar modelos avanzados sin necesidad de
   instalar pesos, configurar entornos complejos o gestionar almacenamiento.

3. **Mayor calidad y robustez:**  
   Los modelos en la nube suelen ser m√°s grandes, m√°s actualizados y con mejor
   rendimiento que los modelos locales livianos (ej: GPT2, LLaMA 7B, Mistral 7B).
   Esto es crucial para tareas como:
   - generaci√≥n de c√≥digo,
   - comprensi√≥n sem√°ntica,
   - clasificaci√≥n de intenci√≥n,
   - generaci√≥n de texto en lenguaje natural.

4. **Escalabilidad y estabilidad:**  
   Los llamados al LLM se realizan de forma consistente y reproducible,
   asegurando que distintos usuarios obtengan resultados comparables.

---

### ¬øPor qu√© no un modelo local?

Modelos locales como Ollama, GPT4All, LLaMA 7B o Mistral 7B, aunque viables para proyectos simples,
no alcanzan el rendimiento necesario para:

- generar c√≥digo correcto de Pandas o Cypher,  
- comprender consultas ambiguas,  
- reescribir respuestas con alta calidad.

Adem√°s, los modelos locales suelen carecer de:
- alineaci√≥n fina para instrucciones,  
- robustez ante consultas ruidosas,  
- soporte multiling√ºe s√≥lido.

Por estas razones, se opta por un modelo en la nube.
"""

def llamar_llm(prompt: str, model: str = modelo_gemini):
    """
    Funci√≥n centralizada para realizar llamadas al LLM.
    Permite estandarizar el uso de Gemini en todo el proyecto.

    Recibe:
      - prompt (str): texto a enviar al modelo
      - model (str): nombre del modelo a utilizar

    Devuelve:
      - texto generado por el LLM
    """
    response = client.models.generate_content(
        model=model,
        contents=[prompt]
    )
    return response.text.strip()

"""## Justificaci√≥n del Modelo de Lenguaje Utilizado (Gemini)

El modelo elegido para este trabajo es **Gemini**, por las siguientes razones:

1. **Excelente comprensi√≥n sem√°ntica en espa√±ol:**  
   El proyecto requiere interpretar consultas naturales sobre productos, ventas, compatibilidad, etc. Gemini muestra un desempe√±o superior en comprensi√≥n en espa√±ol comparado con modelos
   peque√±os locales.

2. **Capacidad para generar c√≥digo:**  
   En la fuente tabular y en la fuente grafo se necesita que el LLM genere:
   - filtros de Pandas,
   - consultas Cypher,
   - expresiones condicionales,
   - verificaciones b√°sicas.

   Gemini demuestra alta precisi√≥n en generaci√≥n de c√≥digo ejecutable.

3. **Consistencia para clasificaci√≥n de intenci√≥n:**  
   El clasificador LLM basado en few-shot funciona con una precisi√≥n perfecta en las pruebas, lo cual confirma la robustez del modelo para tareas de categorizaci√≥n sem√°ntica.

4. **Velocidad y estabilidad:**  
   La API de Gemini permite un tiempo de respuesta adecuado y no requiere gesti√≥n de tokens compleja por parte del usuario.

5. **Integraci√≥n con Python simplificada:**  
   La librer√≠a oficial `google-generativeai` facilita la creaci√≥n de clientes y la interacci√≥n directa con el modelo.

---

### Conclusi√≥n

Gemini es una opci√≥n adecuada para este TP debido a:

- su rendimiento,
- su facilidad de integraci√≥n,
- su soporte multiling√ºe,
- y su capacidad para generar c√≥digo de calidad y respuestas contextuales.

Por estas razones, se adopta como LLM principal del sistema.

## Interacci√≥n conversacional interactiva

Para probar el sistema de manera m√°s realista, se implementa un peque√±o bucle
interactivo en el que el usuario puede escribir preguntas manualmente y finalizar
la sesi√≥n escribiendo `EXIT` o `SALIR`.

En cada turno:

1. El sistema clasifica la intenci√≥n (vectorial / tabular / grafo).
2. Ejecuta el pipeline correspondiente (con b√∫squeda h√≠brida y re-ranking en el caso vectorial).
3. Genera una respuesta con el LLM.
4. Registra la pregunta, la respuesta y la fuente utilizada en el historial de la conversaci√≥n.

Esto permite observar el comportamiento del asistente en un escenario similar al uso real
y verificar que la memoria y la integraci√≥n de componentes funcionan correctamente.
"""

def crear_asistente_conversacional(metodo_clasificador: str = "ml"):
    """
    Asistente conversacional oficial del TP.
    Usa SIEMPRE el asistente avanzado (b√∫squeda h√≠brida + rerank).
    Incluye historial para mantener memoria de la conversaci√≥n.
    """

    historial = []

    def chat(pregunta: str):
        nonlocal historial

        resp = asistente_electro_avanzado(
            pregunta,
            historial=historial,
            metodo_clasificador=metodo_clasificador,
        )

        # Guardamos memoria
        historial.append({
            "pregunta": pregunta,
            "respuesta": resp["respuesta"],
            "fuente": resp["fuente"],
        })

        return resp, historial

    return chat

# Crear asistente conversacional
chat = crear_asistente_conversacional(metodo_clasificador="ml")

print("Asistente Electro (versi√≥n final)")
print("Escrib√≠ tu pregunta en espa√±ol. Escrib√≠ EXIT o SALIR para terminar.\n")

while True:
    pregunta = input("Usuario: ").strip()

    if pregunta.lower() in ["exit", "salir", "quit"]:
        print("\nCerrando la conversaci√≥n. Gracias por usar el asistente. üëã")
        break

    resp, historial = chat(pregunta)

    print(f"\nAsistente ({resp['fuente']}): {resp['respuesta']}\n")
